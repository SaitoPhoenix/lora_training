# CHANGE THIS TO YOUR OWN DATASET
dataset:
  TTS_dataset: "MrDragonFox/Elise" # Replace with the Hugging Face dataset ID of your preprocessed Elise dataset

model:
  details:
    base_model: "canopylabs/orpheus-3b-0.1-pretrained" # Use the pretrained model as your base
    trainable_layers: [] # Layers that should be trainable

  config:
    attn_implementation: "flash_attention_2" # Use Flash Attention 2 for better performance
    torch_dtype: "bfloat16" # Use bfloat16 for mixed precision training
    device_map: "auto" # Automatically handle device placement
    resume_download: true # Resume interrupted downloads
    cache_dir: null # Will be set by ConfigManager

training_config:
  num_train_epochs: 10 # Increased epochs for better voice adaptation
  per_device_train_batch_size: 4 # Increased batch size for better training
  dataloader_num_workers: 1 # Number of processes (adjust for multi-GPU setups with accelerate)
  save_strategy: "steps" # Save based on steps
  save_steps: 50 # Save and evaluate every 50 steps
  eval_strategy: "steps" # Evaluate based on steps
  eval_steps: 50 # Run evaluation every 50 steps
  load_best_model_at_end: true # Load the best model at the end of training
  per_device_eval_batch_size: 4 # Batch size for evaluation
  learning_rate: 5.0e-5 # Slightly lower learning rate for more stable training
  lr_scheduler_type: "cosine" # Use a cosine learning rate scheduler
  logging_steps: 1 # Log training progress every X steps
  gradient_accumulation_steps: 4 # Effective batch size = 16
  warmup_steps: 100 # Number of warmup steps for learning rate scheduler
  max_grad_norm: 1.0 # Maximum gradient norm for gradient clipping
  optim: "adamw_torch" # Optimizer to use
  bf16: true # Enable bfloat16 mixed precision
  gradient_checkpointing: true # Enable gradient checkpointing to save memory
  dataloader_pin_memory: true # Pin memory in dataloader for faster data transfer
  remove_unused_columns: false # Keep all columns in the dataset
  overwrite_output_dir: true # Overwrite output directory
  report_to: "wandb" # Report to Weights & Biases

paths:
  save_folder: "../data/checkpoints" # Folder to save checkpoints and the final LoRA
  wandb_dir: "../data/" # Weights & Biases directory

wandb_config:
  project_name: "tuning-orpheus" # Weights & Biases project name
  run_name: "elise-lora-tuning-10epochs" # Weights & Biases run name

# Token configuration
token_config:
  tokeniser_length: 128256 # Base tokenizer length
  start_of_text: 128000 # Start of text token
  end_of_text: 128009 # End of text token
  start_of_speech: 128257 # Start of speech token (tokeniser_length + 1)
  end_of_speech: 128258 # End of speech token (tokeniser_length + 2)
  start_of_human: 128259 # Start of human token (tokeniser_length + 3)
  end_of_human: 128260 # End of human token (tokeniser_length + 4)
  start_of_ai: 128261 # Start of AI token (tokeniser_length + 5)
  end_of_ai: 128262 # End of AI token (tokeniser_length + 6)
  pad_token: 128263 # Pad token (tokeniser_length + 7)
  audio_tokens_start: 128266 # Start of audio tokens (tokeniser_length + 10)
  max_sequence_length: 1024 # Maximum sequence length for the model

# LoRA configuration
lora_config:
  r: 16 # Reduced rank for better quantization compatibility
  lora_alpha: 32 # Adjusted alpha relative to rank
  lora_dropout: 0.05 # Added small dropout for regularization
  target_modules: # Modules to apply LoRA to
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "down_proj"
    - "up_proj"
  bias: "none" # Bias type for LoRA layers
  modules_to_save: null # Modules to save in full precision
  task_type: "CAUSAL_LM" # Type of task for LoRA
  use_rslora: true # Whether to use RS-LoRA
  inference_mode: false # Whether to use inference mode
  init_lora_weights: "gaussian" # Initialization method for LoRA weights
  use_dora: false # Whether to use DoRA (Domain-adaptive Rank Adaptation)